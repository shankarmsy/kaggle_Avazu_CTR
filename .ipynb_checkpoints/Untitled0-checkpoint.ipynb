{
 "metadata": {
  "name": "",
  "signature": "sha256:ec996fab8e7f4fd760cebfb19a9ccc928965032d96a03ee48315bd32587cd043"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##############################################################################\n",
      "# class, function, generator definitions #####################################\n",
      "##############################################################################\n",
      "\n",
      "class ftrl_proximal(object):\n",
      "    ''' Our main algorithm: Follow the regularized leader - proximal\n",
      "\n",
      "        In short,\n",
      "        this is an adaptive-learning-rate sparse logistic-regression with\n",
      "        efficient L1-L2-regularization\n",
      "\n",
      "        Reference:\n",
      "        http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf\n",
      "    '''\n",
      "\n",
      "    def __init__(self, alpha, beta, L1, L2, D, interaction):\n",
      "        # parameters\n",
      "        self.alpha = alpha\n",
      "        self.beta = beta\n",
      "        self.L1 = L1\n",
      "        self.L2 = L2\n",
      "\n",
      "        # feature related parameters\n",
      "        self.D = D\n",
      "        self.interaction = interaction\n",
      "\n",
      "        # model\n",
      "        # n: squared sum of past gradients\n",
      "        # z: weights\n",
      "        # w: lazy weights\n",
      "        self.n = [0.] * D\n",
      "        self.z = [0.] * D\n",
      "        self.w = {}\n",
      "\n",
      "    def _indices(self, x):\n",
      "        ''' A helper generator that yields the indices in x\n",
      "\n",
      "            The purpose of this generator is to make the following\n",
      "            code a bit cleaner when doing feature interaction.\n",
      "        '''\n",
      "\n",
      "        # first yield index of the bias term\n",
      "        yield 0\n",
      "\n",
      "        # then yield the normal indices\n",
      "        for index in x:\n",
      "            yield index\n",
      "\n",
      "        # now yield interactions (if applicable)\n",
      "        if self.interaction:\n",
      "            D = self.D\n",
      "            L = len(x)\n",
      "\n",
      "            x = sorted(x)\n",
      "            for i in xrange(L):\n",
      "                for j in xrange(i+1, L):\n",
      "                    # one-hot encode interactions with hash trick\n",
      "                    yield abs(hash(str(x[i]) + '_' + str(x[j]))) % D\n",
      "\n",
      "    def predict(self, x):\n",
      "        ''' Get probability estimation on x\n",
      "\n",
      "            INPUT:\n",
      "                x: features\n",
      "\n",
      "            OUTPUT:\n",
      "                probability of p(y = 1 | x; w)\n",
      "        '''\n",
      "\n",
      "        # parameters\n",
      "        alpha = self.alpha\n",
      "        beta = self.beta\n",
      "        L1 = self.L1\n",
      "        L2 = self.L2\n",
      "\n",
      "        # model\n",
      "        n = self.n\n",
      "        z = self.z\n",
      "        w = {}\n",
      "\n",
      "        # wTx is the inner product of w and x\n",
      "        wTx = 0.\n",
      "        for i in self._indices(x):\n",
      "            sign = -1. if z[i] < 0 else 1.  # get sign of z[i]\n",
      "\n",
      "            # build w on the fly using z and n, hence the name - lazy weights\n",
      "            # we are doing this at prediction instead of update time is because\n",
      "            # this allows us for not storing the complete w\n",
      "            if sign * z[i] <= L1:\n",
      "                # w[i] vanishes due to L1 regularization\n",
      "                w[i] = 0.\n",
      "            else:\n",
      "                # apply prediction time L1, L2 regularization to z and get w\n",
      "                w[i] = (sign * L1 - z[i]) / ((beta + sqrt(n[i])) / alpha + L2)\n",
      "\n",
      "            wTx += w[i]\n",
      "\n",
      "        # cache the current w for update stage\n",
      "        self.w = w\n",
      "\n",
      "        # bounded sigmoid function, this is the probability estimation\n",
      "        return 1. / (1. + exp(-max(min(wTx, 35.), -35.)))\n",
      "\n",
      "    def update(self, x, p, y):\n",
      "        ''' Update model using x, p, y\n",
      "\n",
      "            INPUT:\n",
      "                x: feature, a list of indices\n",
      "                p: click probability prediction of our model\n",
      "                y: answer\n",
      "\n",
      "            MODIFIES:\n",
      "                self.n: increase by squared gradient\n",
      "                self.z: weights\n",
      "        '''\n",
      "\n",
      "        # parameter\n",
      "        alpha = self.alpha\n",
      "\n",
      "        # model\n",
      "        n = self.n\n",
      "        z = self.z\n",
      "        w = self.w\n",
      "\n",
      "        # gradient under logloss\n",
      "        g = p - y\n",
      "\n",
      "        # update z and n\n",
      "        for i in self._indices(x):\n",
      "            sigma = (sqrt(n[i] + g * g) - sqrt(n[i])) / alpha\n",
      "            z[i] += g - sigma * w[i]\n",
      "            n[i] += g * g\n",
      "\n",
      "\n",
      "def logloss(p, y):\n",
      "    ''' FUNCTION: Bounded logloss\n",
      "\n",
      "        INPUT:\n",
      "            p: our prediction\n",
      "            y: real answer\n",
      "\n",
      "        OUTPUT:\n",
      "            logarithmic loss of p given y\n",
      "    '''\n",
      "\n",
      "    p = max(min(p, 1. - 10e-15), 10e-15)\n",
      "    return -log(p) if y == 1. else -log(1. - p)\n",
      "\n",
      "\n",
      "def data(path, D):\n",
      "    ''' GENERATOR: Apply hash-trick to the original csv row\n",
      "                   and for simplicity, we one-hot-encode everything\n",
      "\n",
      "        INPUT:\n",
      "            path: path to training or testing file\n",
      "            D: the max index that we can hash to\n",
      "\n",
      "        YIELDS:\n",
      "            ID: id of the instance, mainly useless\n",
      "            x: a list of hashed and one-hot-encoded 'indices'\n",
      "               we only need the index since all values are either 0 or 1\n",
      "            y: y = 1 if we have a click, else we have y = 0\n",
      "    '''\n",
      "\n",
      "    for t, row in enumerate(DictReader(open(path))):\n",
      "        # process id\n",
      "        ID = row['id']\n",
      "        del row['id']\n",
      "\n",
      "        # process clicks\n",
      "        y = 0.\n",
      "        if 'click' in row:\n",
      "            if row['click'] == '1':\n",
      "                y = 1.\n",
      "            del row['click']\n",
      "\n",
      "        # extract date\n",
      "        date = int(row['hour'][4:6])\n",
      "\n",
      "        # turn hour really into hour, it was originally YYMMDDHH\n",
      "        row['hour'] = row['hour'][6:]\n",
      "\n",
      "        # build x\n",
      "        x = []\n",
      "        for key in row:\n",
      "            value = row[key]\n",
      "\n",
      "            # one-hot encode everything with hash trick\n",
      "            index = abs(hash(key + '_' + value)) % D\n",
      "            x.append(index)\n",
      "\n",
      "        yield t, date, ID, x, y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import datetime\n",
      "from csv import DictReader\n",
      "from math import exp, log, sqrt\n",
      "\n",
      "\n",
      "# TL; DR, the main training process starts on line: 250,\n",
      "# you may want to start reading the code from there\n",
      "\n",
      "\n",
      "##############################################################################\n",
      "# parameters #################################################################\n",
      "##############################################################################\n",
      "\n",
      "# A, paths\n",
      "train = 'train.csv'               # path to training file\n",
      "test = 'test.csv'                 # path to testing file\n",
      "submission = 'initial_submission.csv'  # path of to be outputted submission file\n",
      "\n",
      "# B, model\n",
      "alpha = .1  # learning rate\n",
      "beta = 1.   # smoothing parameter for adaptive learning rate\n",
      "L1 = 1.     # L1 regularization, larger value means more regularized\n",
      "L2 = 1.     # L2 regularization, larger value means more regularized\n",
      "\n",
      "# C, feature/hash trick\n",
      "D = 2 ** 20             # number of weights to use\n",
      "interaction = False     # whether to enable poly2 feature interactions\n",
      "\n",
      "# D, training/validation\n",
      "epoch = 1       # learn training data for N passes\n",
      "holdafter = 9   # data after date N (exclusive) are used as validation\n",
      "holdout = None  # use every N training instance for holdout validation\n",
      "\n",
      "\n",
      "##############################################################################\n",
      "# start training #############################################################\n",
      "##############################################################################\n",
      "\n",
      "start = datetime.now()\n",
      "\n",
      "# initialize ourselves a learner\n",
      "learner = ftrl_proximal(alpha, beta, L1, L2, D, interaction)\n",
      "\n",
      "# start training\n",
      "for e in xrange(epoch):\n",
      "    loss = 0.\n",
      "    count = 0\n",
      "\n",
      "    for t, date, ID, x, y in data(train, D):  # data is a generator\n",
      "        #    t: just a instance counter\n",
      "        # date: you know what this is\n",
      "        #   ID: id provided in original data\n",
      "        #    x: features\n",
      "        #    y: label (click)\n",
      "\n",
      "        # step 1, get prediction from learner\n",
      "        p = learner.predict(x)\n",
      "\n",
      "        if (holdafter and date > holdafter) or (holdout and t % holdout == 0):\n",
      "            # step 2-1, calculate validation loss\n",
      "            #           we do not train with the validation data so that our\n",
      "            #           validation loss is an accurate estimation\n",
      "            #\n",
      "            # holdafter: train instances from day 1 to day N\n",
      "            #            validate with instances from day N + 1 and after\n",
      "            #\n",
      "            # holdout: validate with every N instance, train with others\n",
      "            loss += logloss(p, y)\n",
      "            count += 1\n",
      "        else:\n",
      "            # step 2-2, update learner with label (click) information\n",
      "            learner.update(x, p, y)\n",
      "\n",
      "    print('Epoch %d finished, validation logloss: %f, elapsed time: %s' % (\n",
      "        e, loss/count, str(datetime.now() - start)))\n",
      "\n",
      "\n",
      "##############################################################################\n",
      "# start testing, and build Kaggle's submission file ##########################\n",
      "##############################################################################\n",
      "\n",
      "with open(submission, 'w') as outfile:\n",
      "    outfile.write('id,click\\n')\n",
      "    for t, date, ID, x, y in data(test, D):\n",
      "        p = learner.predict(x)\n",
      "        outfile.write('%s,%s\\n' % (ID, str(p)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Epoch 0 finished, validation logloss: 0.693147, elapsed time: 1:56:06.837423\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from blaze import "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test=Data('train.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import SGDClassifier\n",
      "import multiprocessing\n",
      "multiprocessing.pool=4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numt = into(np.ndarray, test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "?into"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}